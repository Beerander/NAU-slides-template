\documentclass[a4paper, 12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[numbers, compress]{natbib}
\usepackage[margin=1in]{geometry}
% \usepackage{fontspec}
\usepackage{url}
% \setmainfont{Times New Roman}
% \usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}


\title{\textbf{PLATE - video draft}}
\author{Shengqiang Zhang}
\date{\today}

\begin{document}
\maketitle

\textbf{P1} Hello, everyone. It's my pleasure to share our work in ACL 2022. I am Shengqiang. This work was finished with Xingxing, Hangbo, and Furu during my internship at Microsoft Research Asia. 

\textbf{P3-4} First, I will simply introduce the background of our work. 
[TURN]
Our work is about summarization distillation.
Nowadays, large pre-trained language models like BART, T5, bring significant performance improvements on many tasks.
However, the large amount of parameters also make them slow for online inference.
[TURN]
Our work of summarization distillation aims to distill these large summarization models into smaller ones with minimal loss in performance.

\textbf{P5} 
Knowledge distillation is a common method to compress large models. It usually leverage the output of a large teacher model to guide the training of a small student model.
As to summarization distillation, there is a typical method for sequence-level knowledge distillation, called pseudo-labeling. 
As the below figure shows, the teacher model uses beam search to generate pseudo summaries for all documents in the training set. Then the resulting pairs of document and pseudo summaries are used to train the student model.

\textbf{P6}
Our proposed method is motivated by two biases observed in pseudo labels. The first is the copy bias. We observe that pseudo summaries usually copy more continuous spans from source document than reference summaries. Another bias is the leading bias which means pseudo summaries tend to just summarize the leading part of a document.

\textbf{P7}
We have a cross attention visualization of an example to intuitively show those two biases. 
The horizontal and vertical axes in the figure represent the token index in target summary and in source document, respectively. As can be seen from the left figure, the attention weights form three "lines", which indicates very time the decoder predicts the next word, its attention points to the next word in the input document. That may be the reason why multiple continuous spans of text are copied.
Another phenomenon we observe is that all high-value attention weights concentrate on the first 200 words in the input document, which reflects the leading bias. In either case, the attention distribution is too sharp, which means our teacher model is over-confident.
We propose a simple and effective method PLATE which can smooth attention distributions of teacher models, leading to less copy bias and leading bias, as the right figure shows.

\textbf{P8-P9}
OK. Let's take a look at our proposed method PLATE.
[TURN]
The equation one is used to calculate attention in a seq2seq model. $\tau$ is the attention temperature. $\tau$ equals the square root of $\lambda$d. $\lambda$ is set to 1 in vanilla Transformer.
Our method is to re-scale the attention temperature to a higher value during the process of teacher generating the pseudo summaries while keeping $\lambda = 1$ during the teacher's training and the student's training.
In practice, we find $\lambda$ equals to 1.5 or 2.0 usually works well. 
To encourage the teacher model to generate pseudo summaries with more diversity, we further propose to use a random $\lambda$ for each document. $\lambda$ is sampled from a uniform distribution. We typically set a to 1.0 and b to 2.0.

\textbf{P10-P11}
Our method is quite simple, but it's very effective. Let's see the experiments. 
[TURN]
Our experiments are conducted on three document summarization datasets: CNN/DailyMail, XSum, and New York Times.
We use the BART Large as the teacher model which has 12 encoder layers and 12 decoder layers. We have four kinds of student models, including BART with three different decoder layers and Transformer.

\textbf{P12}
Here is the experiment results on the CNNDM dataset. The results of our student models are presented from the third block in the table.
Gold means training the student model directly with the gold train data. Regular means distillation with the regular pseudo-labeling method with normal attention temperature. The following three lines are distillation with our method. We can observe our method consistently outperform its counterpart with the normal attention temperature across all kinds of student models. 
We can see that although random temperature based method is not as good as our best fixed-temperature method, it in general produces decent results. Therefore, we recommend using this method when the computing budget is limited.

\textbf{P13}
The results on another two datasets XSum and NYT follow the same trend as CNNDM dataset. Our method PLATE consistently outperform the baseline regular pseudo-labeling method. 

\textbf{P14}
As we know, there are three types of attention in the seq2seq attention modules and we can scale attention temperature for all of them or some of them. We want to know which type of attention is the most crucial to the performance improvements.
As can be seen from the table, using large attention temperature coefficients (2.0) for all three types of attention modules
leads to the best result. When setting the coefficient of the cross attention module to 1.0, the ROUGE scores drop most.
Besides, the attention temperature of the decoder self-attention is also crucial but not as important as the cross-attention.

\textbf{P15}
We compare our method with the sampling method because sampling based methods can produce more diverse and richer outputs than its beam search based counterpart. But as the table shows, sampling based method can't help the summarization distillation. 
Besides the attention temperature, we can also tune the temperature $T$ in the decoder output softmax layer. We find that with a proper $T$ during pseudo label generation, the resulting student model slightly outperforms the baseline student model with regular pseudo labeling method on ROUGE-2/L (see Table 5), but worse than PLATE.


\textbf{P16}
Our method is simple but very effective. So why does our distillation method work?

\textbf{P18}
We first analyze the length and novel n-grams of the generated summaries. 
[TURN]
As shown in the table, when using a large $\lambda$, pseudo summaries are shorter and contain a larger portion of novel n-grams, which means that pseudo summaries are more concise and abstractive.
[TURN]
The styles of summaries from student models are similar with their teachers. Concise and abstractive teachers lead to concise and abstractive students.

\textbf{P19-P20}
We have shown earlier that with higher attention temperatures, cross-attention modules of teacher models can attend to more later parts in documents. [TURN]
We observe that students behave similarly. With higher attention temperatures, they also pay more attention to diverse parts in documents.

\textbf{P21}
To obtain corpus-level statistics, we further calculate the \emph{evident} cross-attention weight distributions of the teacher when generating pseudo labels on the training set of CNNDM.
The below figure shows the distributions of evident cross attention weights when teachers generate pseudo labels with different attention temperatures w.r.t. token positions.
We can see, compared to the teacher with normal attention temperature, teachers with higher attention temperatures attend less on the heading parts of documents while more on the tail parts of documents. 

\textbf{P22}
Lastly, we provide an in-depth explanation according to Xu's work in EMNLP 2020. They find the model's high entropy of attention distribution strongly correlates with the high prediction entropy. When the model has a low prediction entropy, the decoder tends to copy from source documents; when the model has a high prediction entropy, the decoder tends to generate novel bigrams.
[TURN]
This finding can be applied to explain our method. Our method with a higher attention temperature makes attention distribution of the teacher model smoother, leading to a higher entropy of attention distributions, which results in a higher prediction entropy. Therefore, the model with higher attention temperature tends to copy less and generate more novel tokens.

\textbf{P23}
In summary, we propose a simple but very effective extensions of pseudo-labeling method {\sc Plate} for abstractive summarization distillation. Experiments on three datasets demonstrate our method can consistently outperform the vanilla pseudo-labeling method. And further empirical analysis shows that, with our method, both teacher's pseudo summaries and student's summaries are more concise and abstractive.

That's all the content of our paper. Thanks for your listening!
\end{document}